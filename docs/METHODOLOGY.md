# üìä Metodolog√≠a del Proyecto

## Modelo Predictivo de Riesgo de Sustituci√≥n Laboral por IA

**Autor:** Carlos Pulido Rosas  
**Tesis:** Maestr√≠a en Ciencias de los Datos - CUCEA

---

## 1. Marco Metodol√≥gico

### 1.1 Tipo de Investigaci√≥n

- **Tipo:** Investigaci√≥n cuantitativa con enfoque predictivo
- **Alcance:** Descriptivo-explicativo-predictivo
- **Dise√±o:** No experimental, transversal con proyecci√≥n temporal
- **Enfoque:** Data Science y Machine Learning aplicado

### 1.2 Pregunta de Investigaci√≥n

**¬øCu√°les ocupaciones en Jalisco tienen mayor riesgo de sustituci√≥n por Inteligencia Artificial en el per√≠odo 2025-2030, considerando caracter√≠sticas ocupacionales y tendencias socioecon√≥micas?**

### 1.3 Hip√≥tesis

**H1:** Ocupaciones con alto grado de rutinizaci√≥n de tareas tienen mayor probabilidad de automatizaci√≥n.

**H2:** El nivel educativo requerido est√° inversamente correlacionado con el riesgo de automatizaci√≥n.

**H3:** Sectores de servicios administrativos y manufactura presentan mayor riesgo que sectores creativos y de alta especializaci√≥n.

---

## 2. Dise√±o Metodol√≥gico

### 2.1 Fases del Proyecto

```
Fase 1: Recolecci√≥n de Datos (Semanas 1-3)
‚îú‚îÄ‚îÄ Obtenci√≥n datos O*NET
‚îú‚îÄ‚îÄ Descarga ENOE Jalisco
‚îú‚îÄ‚îÄ B√∫squeda fuentes complementarias
‚îî‚îÄ‚îÄ Documentaci√≥n de fuentes

Fase 2: Preprocesamiento (Semanas 4-6)
‚îú‚îÄ‚îÄ Limpieza de datos
‚îú‚îÄ‚îÄ Integraci√≥n de fuentes
‚îú‚îÄ‚îÄ Normalizaci√≥n
‚îú‚îÄ‚îÄ Manejo de valores faltantes
‚îî‚îÄ‚îÄ Validaci√≥n de calidad

Fase 3: An√°lisis Exploratorio (Semanas 7-9)
‚îú‚îÄ‚îÄ Estad√≠sticas descriptivas
‚îú‚îÄ‚îÄ An√°lisis univariado
‚îú‚îÄ‚îÄ An√°lisis bivariado
‚îú‚îÄ‚îÄ Identificaci√≥n de patrones
‚îî‚îÄ‚îÄ Visualizaciones iniciales

Fase 4: Feature Engineering (Semanas 10-12)
‚îú‚îÄ‚îÄ Creaci√≥n de √≠ndices
‚îú‚îÄ‚îÄ Transformaci√≥n de variables
‚îú‚îÄ‚îÄ Selecci√≥n de features
‚îî‚îÄ‚îÄ Ingenier√≠a de caracter√≠sticas

Fase 5: Modelado (Semanas 13-16)
‚îú‚îÄ‚îÄ Selecci√≥n de algoritmos
‚îú‚îÄ‚îÄ Entrenamiento de modelos
‚îú‚îÄ‚îÄ Validaci√≥n cruzada
‚îú‚îÄ‚îÄ Optimizaci√≥n de hiperpar√°metros
‚îî‚îÄ‚îÄ Ensamble de modelos

Fase 6: Evaluaci√≥n y Resultados (Semanas 17-18)
‚îú‚îÄ‚îÄ M√©tricas de desempe√±o
‚îú‚îÄ‚îÄ Interpretaci√≥n de resultados
‚îú‚îÄ‚îÄ Validaci√≥n con expertos
‚îî‚îÄ‚îÄ An√°lisis de sensibilidad

Fase 7: Documentaci√≥n (Semanas 19-20)
‚îú‚îÄ‚îÄ Reporte t√©cnico
‚îú‚îÄ‚îÄ Visualizaciones finales
‚îú‚îÄ‚îÄ Presentaci√≥n de resultados
‚îî‚îÄ‚îÄ Tesis escrita
```

---

## 3. Fuentes de Datos

### 3.1 Datos Primarios

#### O*NET Database
- **Descripci√≥n:** Base de datos de caracter√≠sticas ocupacionales
- **Variables clave:**
  - Habilidades requeridas (100+ categor√≠as)
  - Conocimientos necesarios
  - Actividades laborales
  - Contexto laboral
  - Caracter√≠sticas del trabajo
  
#### ENOE - Jalisco
- **Descripci√≥n:** Encuesta de empleo en Jalisco
- **Variables clave:**
  - Ocupaci√≥n (clasificaci√≥n SINCO)
  - N√∫mero de trabajadores
  - Salarios
  - Nivel educativo
  - Sector econ√≥mico
  - Municipio

### 3.2 Datos Secundarios

#### Estudios de Automatizaci√≥n
- Frey & Osborne (2013) - Probabilidades de automatizaci√≥n
- McKinsey Global Institute - √çndices de automatizaci√≥n
- World Economic Forum - Future of Jobs Report

#### Datos Econ√≥micos
- PIB sectorial Jalisco (INEGI)
- Inversi√≥n en tecnolog√≠a por sector
- Tendencias de empleo hist√≥rico

---

## 4. Variables del Estudio

### 4.1 Variable Dependiente

**Riesgo de Automatizaci√≥n (automation_risk)**
- Tipo: Num√©rica continua [0, 1]
- Definici√≥n: Probabilidad de que una ocupaci√≥n sea automatizada
- Categorizaci√≥n:
  - Alto: > 0.70
  - Medio: 0.30 - 0.70
  - Bajo: < 0.30

### 4.2 Variables Independientes

#### Caracter√≠sticas de Tareas (Task-based)
1. **task_routine_index** - √çndice de rutinizaci√≥n (0-100)
2. **task_cognitive_demand** - Demanda cognitiva (0-100)
3. **task_manual** - Intensidad manual (0-100)
4. **task_social_interaction** - Interacci√≥n social (0-100)
5. **task_creativity** - Creatividad requerida (0-100)

#### Habilidades Requeridas
1. **skills_technical** - Habilidades t√©cnicas (0-100)
2. **skills_analytical** - Pensamiento anal√≠tico (0-100)
3. **skills_interpersonal** - Habilidades sociales (0-100)
4. **skills_management** - Gesti√≥n (0-100)

#### Contexto Laboral
1. **education_level** - Nivel educativo (ordinal)
   - 1: Sin educaci√≥n formal
   - 2: Primaria
   - 3: Secundaria
   - 4: Preparatoria
   - 5: Universidad
   - 6: Posgrado

2. **avg_salary** - Salario promedio (MXN/mes)
3. **workers_count** - N√∫mero de trabajadores
4. **sector** - Sector econ√≥mico (categ√≥rica)

#### Variables de Control
1. **year** - A√±o de referencia
2. **region** - Regi√≥n en Jalisco
3. **company_size** - Tama√±o de empresa

---

## 5. Proceso de An√°lisis con PySpark

### 5.1 Arquitectura de Procesamiento

```python
# Pipeline de procesamiento
raw_data ‚Üí cleaning ‚Üí feature_engineering ‚Üí modeling ‚Üí evaluation
```

### 5.2 Configuraci√≥n de Spark

```python
spark = SparkSession.builder \
    .appName("AI_Automation_Risk_Analysis") \
    .config("spark.driver.memory", "8g") \
    .config("spark.executor.memory", "8g") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()
```

### 5.3 Carga de Datos

```python
# O*NET data
onet_df = spark.read.csv("data/raw/onet_occupations.csv", 
                         header=True, inferSchema=True)

# ENOE Jalisco
enoe_df = spark.read.csv("data/raw/enoe_jalisco.csv",
                         header=True, inferSchema=True,
                         encoding='latin1')

# Join datasets
combined_df = onet_df.join(enoe_df, 
                           onet_df.soc_code == enoe_df.occupation_code,
                           'inner')
```

### 5.4 Preprocesamiento

```python
# Convertir a pyspark.pandas
import pyspark.pandas as ps
df_ps = combined_df.pandas_api()

# Limpieza
df_clean = df_ps.dropna(subset=['occupation_name', 'workers_count'])
df_clean = df_clean[df_clean['workers_count'] > 0]

# Normalizaci√≥n
from pyspark.ml.feature import StandardScaler
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
```

### 5.5 Feature Engineering

```python
# √çndice de rutinizaci√≥n
df_ps['routine_index'] = (
    df_ps['task_repetitive'] * 0.4 +
    df_ps['task_predictable'] * 0.3 +
    df_ps['task_structured'] * 0.3
)

# √çndice de automatizaci√≥n base (m√©todo Frey-Osborne)
df_ps['automation_base'] = (
    df_ps['routine_index'] * 0.5 -
    df_ps['creativity'] * 0.25 -
    df_ps['social_intelligence'] * 0.25
)

# Categorizaci√≥n de riesgo
def categorize_risk(score):
    if score >= 0.70:
        return 'Alto'
    elif score >= 0.30:
        return 'Medio'
    else:
        return 'Bajo'

df_ps['risk_category'] = df_ps['automation_base'].apply(categorize_risk)
```

---

## 6. Modelado Predictivo

### 6.1 Algoritmos a Utilizar

#### Modelos de Clasificaci√≥n
1. **Random Forest Classifier** (Spark MLlib)
   - Ventaja: Interpreta importancia de features
   - Hiperpar√°metros: numTrees, maxDepth

2. **Gradient Boosting Trees** (Spark MLlib)
   - Ventaja: Alto desempe√±o predictivo
   - Hiperpar√°metros: maxIter, stepSize

3. **Logistic Regression** (Spark MLlib)
   - Ventaja: Baseline interpretable
   - Hiperpar√°metros: maxIter, regParam

#### Modelos de Regresi√≥n
1. **Linear Regression** (Spark MLlib)
   - Para predecir probabilidad continua

2. **Random Forest Regressor**
   - Para predicciones robustas

### 6.2 Pipeline de Modelado

```python
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import VectorAssembler, StringIndexer

# Preparar features
feature_cols = ['routine_index', 'education_level', 'avg_salary',
                'task_cognitive', 'skills_technical']

assembler = VectorAssembler(inputCols=feature_cols, 
                            outputCol="features")

# Indexar target
indexer = StringIndexer(inputCol="risk_category", 
                       outputCol="label")

# Modelo
rf = RandomForestClassifier(featuresCol="features",
                            labelCol="label",
                            numTrees=100,
                            maxDepth=10)

# Pipeline completo
pipeline = Pipeline(stages=[assembler, indexer, rf])
```

### 6.3 Validaci√≥n

```python
# Split train/test
train_df, test_df = df_spark.randomSplit([0.8, 0.2], seed=42)

# Entrenar
model = pipeline.fit(train_df)

# Predecir
predictions = model.transform(test_df)

# Evaluar
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

evaluator = MulticlassClassificationEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="accuracy"
)

accuracy = evaluator.evaluate(predictions)
print(f"Accuracy: {accuracy:.3f}")
```

---

## 7. M√©tricas de Evaluaci√≥n

### 7.1 M√©tricas de Clasificaci√≥n

- **Accuracy**: Precisi√≥n general
- **Precision**: Por clase (Alto/Medio/Bajo)
- **Recall**: Sensibilidad por clase
- **F1-Score**: Media arm√≥nica
- **Matriz de Confusi√≥n**: Errores por clase
- **ROC-AUC**: Curva ROC multiclase

### 7.2 M√©tricas de Regresi√≥n

- **RMSE**: Error cuadr√°tico medio
- **MAE**: Error absoluto medio
- **R¬≤**: Coeficiente de determinaci√≥n

### 7.3 Validaci√≥n Externa

- Comparaci√≥n con estudios previos (Frey-Osborne)
- Validaci√≥n con expertos del mercado laboral
- An√°lisis de casos espec√≠ficos

---

## 8. An√°lisis de Resultados

### 8.1 Interpretaci√≥n de Importancia de Features

```python
# Feature importance
feature_importance = model.stages[-1].featureImportances

# Ordenar
importance_df = ps.DataFrame({
    'feature': feature_cols,
    'importance': feature_importance.toArray()
}).sort_values('importance', ascending=False)
```

### 8.2 An√°lisis de Sensibilidad

- Variaci√≥n de par√°metros
- An√°lisis "what-if"
- Escenarios optimista/pesimista

### 8.3 Proyecciones Temporales

```python
# Proyecci√≥n 2025-2030
years = range(2025, 2031)
for year in years:
    projected_risk = base_risk * (1 + annual_growth_rate) ** (year - 2025)
```

---

## 9. Visualizaciones

### 9.1 Visualizaciones Exploratorias

1. Distribuci√≥n de ocupaciones por sector
2. Correlaciones entre variables
3. Box plots por nivel educativo
4. Scatter plots salario vs riesgo

### 9.2 Visualizaciones de Resultados

1. Mapa de calor: Riesgo por sector y educaci√≥n
2. Treemap: Impacto por n√∫mero de trabajadores
3. Serie temporal: Proyecciones 2025-2030
4. Red: Relaciones entre habilidades
5. Mapa geogr√°fico: Jalisco por municipio

### 9.3 Dashboard Interactivo

```python
import plotly.express as px
import plotly.graph_objects as go

# Dashboard con Plotly
fig = go.Figure()
fig.add_trace(go.Bar(...))
fig.show()
```

---

## 10. Consideraciones √âticas

### 10.1 Privacidad de Datos

- Datos agregados (no individuales)
- Anonimizaci√≥n de informaci√≥n sensible
- Cumplimiento con INAI

### 10.2 Sesgo en Datos

- Verificaci√≥n de representatividad
- An√°lisis de equidad por g√©nero
- Consideraci√≥n de grupos vulnerables

### 10.3 Uso Responsable de Resultados

- Recomendaciones de pol√≠tica p√∫blica
- Enfoque en reconversi√≥n laboral
- No estigmatizaci√≥n de ocupaciones

---

## 11. Limitaciones del Estudio

### 11.1 Limitaciones de Datos

- Actualizaci√≥n de datos O*NET (anual)
- Cobertura ENOE (no todas las ocupaciones)
- Cambio tecnol√≥gico acelerado

### 11.2 Limitaciones Metodol√≥gicas

- Proyecciones basadas en tendencias actuales
- Factores externos no considerados
- Alcance geogr√°fico limitado a Jalisco

### 11.3 Limitaciones T√©cnicas

- Capacidad computacional
- Disponibilidad de datos hist√≥ricos
- Modelos simplificados de adopci√≥n tecnol√≥gica

---

## 12. Cronograma

|  Fase  | Duraci√≥n  |      Entregables       |
|--------|-----------|------------------------|
| Fase 1 | 3 semanas | Datasets integrados    |
| Fase 2 | 3 semanas | Datos limpios          |
| Fase 3 | 3 semanas | Reporte exploratorio   |
| Fase 4 | 3 semanas | Features engineered    |
| Fase 5 | 4 semanas | Modelos entrenados     |
| Fase 6 | 2 semanas | Evaluaci√≥n completa    |
| Fase 7 | 2 semanas | Tesis y presentaci√≥n   |

**Total:** 20 semanas (~5 meses)

---

## 13. Referencias Metodol√≥gicas

### Art√≠culos Clave

1. Frey, C. B., & Osborne, M. A. (2017). The future of employment: How susceptible are jobs to computerisation? *Technological Forecasting and Social Change*, 114, 254-280.

2. Arntz, M., Gregory, T., & Zierahn, U. (2016). The risk of automation for jobs in OECD countries: A comparative analysis. *OECD Social, Employment and Migration Working Papers*, No. 189.

3. Autor, D. H., Levy, F., & Murnane, R. J. (2003). The skill content of recent technological change: An empirical exploration. *The Quarterly Journal of Economics*, 118(4), 1279-1333.

### Recursos T√©cnicos

- PySpark Documentation: https://spark.apache.org/docs/latest/api/python/
- O*NET Database: https://www.onetcenter.org/
- INEGI Metodolog√≠a ENOE: https://www.inegi.org.mx/programas/enoe/

---

## 14. Contribuci√≥n Esperada

### 14.1 Contribuci√≥n Acad√©mica

- Aplicaci√≥n de Big Data a mercado laboral mexicano
- Metodolog√≠a replicable para otras entidades
- Integraci√≥n de fuentes heterog√©neas

### 14.2 Contribuci√≥n Pr√°ctica

- Herramienta de diagn√≥stico para pol√≠tica p√∫blica
- Identificaci√≥n de necesidades de capacitaci√≥n
- Planeaci√≥n educativa basada en evidencia

### 14.3 Contribuci√≥n Tecnol√≥gica

- Pipeline escalable con PySpark
- C√≥digo abierto y documentado
- Dashboard interactivo para stakeholders

--- Fin del Documento ---