"""
Main Script
===========
Script principal para ejecutar el an√°lisis completo de automatizaci√≥n laboral.

Uso:
    python main.py --mode sample
    python main.py --mode real --occupation-data data/raw/onet.csv --employment-data data/raw/enoe.csv

Autor: Carlos Pulido Rosas
Proyecto: Modelo Predictivo de Sustituci√≥n Laboral por IA - Jalisco
"""

import argparse
import sys
import os
import logging
import warnings
from datetime import datetime

# Suprimir warnings de PySpark y bibliotecas relacionadas
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', module='pyspark')
warnings.filterwarnings('ignore', module='py4j')

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Importar m√≥dulos del proyecto
from data_loader import (
    create_spark_session,
    load_sample_data,
    load_onet_occupations,
    load_enoe_jalisco,
    convert_to_pandas_api,
    save_dataset
)

from data_preprocessing import (
    preprocess_pipeline,
    validate_data_quality
)

from feature_engineering import (
    feature_engineering_pipeline
)

from automation_analyzer import (
    AutomationRiskAnalyzer,
    generate_risk_report
)

from visualizations import create_dashboard


def parse_arguments():
    """
    Parsea argumentos de l√≠nea de comandos.
    
    Returns:
    --------
    argparse.Namespace
        Argumentos parseados
    """
    parser = argparse.ArgumentParser(
        description='An√°lisis de Riesgo de Automatizaci√≥n Laboral en Jalisco',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Ejemplos de uso:
  # Usar datos simulados (recomendado para pruebas)
  python main.py --mode sample --n-occupations 200

  # Usar datos reales
  python main.py --mode real \\
    --occupation-data data/raw/onet/Occupation_Data.txt \\
    --employment-data data/raw/enoe_jalisco.csv

  # Especificar directorio de salida
  python main.py --mode sample --output outputs/run_2025_11_26

  # Sin visualizaciones (m√°s r√°pido)
  python main.py --mode sample --no-visualizations
        '''
    )
    
    # Modo de ejecuci√≥n
    parser.add_argument(
        '--mode',
        type=str,
        choices=['sample', 'real'],
        default='sample',
        help='Modo de ejecuci√≥n: "sample" (datos simulados) o "real" (datos reales)'
    )
    
    # Datos de entrada (solo para modo real)
    parser.add_argument(
        '--occupation-data',
        type=str,
        help='Ruta a archivo de datos de ocupaciones (O*NET)'
    )
    
    parser.add_argument(
        '--employment-data',
        type=str,
        help='Ruta a archivo de datos de empleo (ENOE Jalisco)'
    )
    
    # Configuraci√≥n de datos simulados
    parser.add_argument(
        '--n-occupations',
        type=int,
        default=200,
        help='N√∫mero de ocupaciones simuladas (solo modo sample)'
    )
    
    # Directorios de salida
    parser.add_argument(
        '--output',
        type=str,
        default='outputs',
        help='Directorio base para guardar resultados'
    )
    
    # Opciones de procesamiento
    parser.add_argument(
        '--no-visualizations',
        action='store_true',
        help='Saltar generaci√≥n de visualizaciones (m√°s r√°pido)'
    )
    
    parser.add_argument(
        '--no-model',
        action='store_true',
        help='Saltar entrenamiento de modelo predictivo'
    )
    
    parser.add_argument(
        '--auto-download',
        action='store_true',
        help='Descargar datos autom√°ticamente si no existen'
    )
    
    # Configuraci√≥n de Spark
    parser.add_argument(
        '--memory',
        type=str,
        default='8g',
        help='Memoria asignada a Spark (ej: 4g, 8g, 16g)'
    )
    
    return parser.parse_args()


def validate_inputs(args):
    """
    Valida los argumentos de entrada.
    
    Parameters:
    -----------
    args : argparse.Namespace
        Argumentos parseados
        
    Returns:
    --------
    bool
        True si v√°lido, False en caso contrario
    """
    if args.mode == 'real':
        # Si auto-download est√° activado, no validar archivos ahora
        if args.auto_download:
            return True
            
        if not args.occupation_data:
            logger.error("Modo 'real' requiere --occupation-data (o usar --auto-download)")
            return False
        
        if not args.employment_data:
            logger.error("Modo 'real' requiere --employment-data (o usar --auto-download)")
            return False
        
        if not os.path.exists(args.occupation_data):
            logger.error(f"Archivo no encontrado: {args.occupation_data}")
            logger.info("üí° Usa --auto-download para descargar autom√°ticamente")
            return False
        
        if not os.path.exists(args.employment_data):
            logger.error(f"Archivo no encontrado: {args.employment_data}")
            return False
    
    return True


def main():
    """
    Funci√≥n principal del script.
    """
    # Banner
    print("\n" + "="*80)
    print("AN√ÅLISIS DE RIESGO DE AUTOMATIZACI√ìN LABORAL")
    print("Modelo Predictivo para Jalisco, M√©xico (2025-2030)")
    print("Autor: Carlos Pulido Rosas - CUCEA, UdeG")
    print("="*80 + "\n")
    
    # Parsear argumentos
    args = parse_arguments()
    
    # Validar inputs
    if not validate_inputs(args):
        sys.exit(1)
    
    # Crear directorios de salida
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_base = os.path.abspath(args.output)  # Usar ruta absoluta
    output_processed = os.path.join(output_base, 'processed')
    output_reports = os.path.join(output_base, 'reports')
    output_viz = os.path.join(output_base, 'visualizations')
    output_models = os.path.join(output_base, 'models')
    
    os.makedirs(output_processed, exist_ok=True)
    os.makedirs(output_reports, exist_ok=True)
    os.makedirs(output_viz, exist_ok=True)
    os.makedirs(output_models, exist_ok=True)
    
    logger.info(f"Directorios de salida creados en: {output_base}")
    logger.info(f"  - Reports: {output_reports}")
    
    try:
        # ====================================================================
        # PASO 1: INICIALIZACI√ìN
        # ====================================================================
        logger.info("\n" + "="*80)
        logger.info("PASO 1/7: INICIALIZACI√ìN")
        logger.info("="*80)
        
        # Crear Spark Session
        spark = create_spark_session(
            app_name="AI_Automation_Risk_Jalisco",
            memory=args.memory
        )
        
        # Suprimir logs verbosos de Spark
        spark.sparkContext.setLogLevel("ERROR")
        
        # ====================================================================
        # PASO 2: CARGA DE DATOS
        # ====================================================================
        logger.info("\n" + "="*80)
        logger.info("PASO 2/7: CARGA DE DATOS")
        logger.info("="*80)
        
        if args.mode == 'sample':
            logger.info(f"Modo: Datos simulados (n={args.n_occupations})")
            df_spark = load_sample_data(spark, n_occupations=args.n_occupations)
        
        elif args.mode == 'real':
            logger.info("Modo: Datos reales")
            
            # Descargar datos autom√°ticamente si se solicit√≥
            if args.auto_download:
                from data_loader import auto_download_data
                paths = auto_download_data(force_download=False)
                
                # Actualizar rutas si no se proporcionaron
                if not args.occupation_data:
                    args.occupation_data = os.path.join(paths['onet_dir'], 'Occupation Data.txt')
                if not args.employment_data:
                    args.employment_data = paths['enoe_file']
            
            # Cargar O*NET
            logger.info(f"  Cargando O*NET desde: {args.occupation_data}")
            df_onet = load_onet_occupations(spark, args.occupation_data)
            
            # Cargar ENOE
            logger.info(f"  Cargando ENOE desde: {args.employment_data}")
            df_enoe = load_enoe_jalisco(spark, args.employment_data)
            
            # Integrar (simplificado - en producci√≥n usar mapeo SOC-SINCO)
            df_spark = df_onet.join(df_enoe, how='inner')
            logger.info("‚úì Datos integrados")
        
        # Convertir a pyspark.pandas
        df = convert_to_pandas_api(df_spark)
        logger.info(f"‚úì Dataset cargado: {df.shape[0]:,} ocupaciones, {df.shape[1]} columnas")
        
        # ====================================================================
        # PASO 3: PREPROCESAMIENTO
        # ====================================================================
        logger.info("\n" + "="*80)
        logger.info("PASO 3/7: PREPROCESAMIENTO")
        logger.info("="*80)
        
        df_clean = preprocess_pipeline(df, config={
            'handle_missing': 'auto',
            'remove_duplicates': True,
            'filter_outliers': False
        })
        
        logger.info(f"‚úì Datos limpios: {df_clean.shape[0]:,} ocupaciones")
        
        # ====================================================================
        # PASO 4: FEATURE ENGINEERING
        # ====================================================================
        logger.info("\n" + "="*80)
        logger.info("PASO 4/7: FEATURE ENGINEERING")
        logger.info("="*80)
        
        df_features = feature_engineering_pipeline(df_clean, config={
            'create_indices': True,
            'create_categories': True,
            'create_ratios': True,
            'create_temporal': True,
            'create_sector_agg': True
        })
        
        logger.info(f"‚úì Features creados: {df_features.shape[1]} columnas totales")
        
        # ====================================================================
        # PASO 5: AN√ÅLISIS DE RIESGO
        # ====================================================================
        logger.info("\n" + "="*80)
        logger.info("PASO 5/7: AN√ÅLISIS DE RIESGO DE AUTOMATIZACI√ìN")
        logger.info("="*80)
        
        # Crear analizador
        analyzer = AutomationRiskAnalyzer()
        
        # Calcular riesgo
        df_risk = analyzer.calculate_automation_risk(
            df_features,
            method='frey_osborne'
        )
        
        # Categorizar
        df_risk = analyzer.categorize_risk(df_risk, thresholds=(0.30, 0.70))
        
        # An√°lisis por dimensiones
        logger.info("\nAn√°lisis por dimensiones:")
        
        sector_analysis = analyzer.analyze_by_sector(df_risk)
        education_analysis = analyzer.analyze_by_education(df_risk)
        impact = analyzer.calculate_economic_impact(df_risk)
        
        logger.info(f"  Trabajadores en alto riesgo: {impact['workers_high_risk']:,} ({impact['pct_workers_at_risk']:.1f}%)")
        
        # ====================================================================
        # PASO 6: VISUALIZACIONES
        # ====================================================================
        if not args.no_visualizations:
            logger.info("\n" + "="*80)
            logger.info("PASO 6/7: GENERACI√ìN DE VISUALIZACIONES")
            logger.info("="*80)
            
            # Dashboard completo (14 visualizaciones)
            create_dashboard(df_risk, output_dir=output_viz)
            
            logger.info(f"‚úì Visualizaciones guardadas en: {output_viz}")
        else:
            logger.info("\nPASO 6/7: VISUALIZACIONES - OMITIDAS")
        
        # ====================================================================
        # PASO 7: INFERENCIA ESTAD√çSTICA
        # ====================================================================
        logger.info("\n" + "="*80)
        logger.info("PASO 7/8: INFERENCIA ESTAD√çSTICA")
        logger.info("="*80)
        
        from statistical_inference import StatisticalInference
        
        # Crear instancia y ejecutar todas las pruebas
        inference = StatisticalInference(df_risk)
        inference_results = inference.run_all_inference_tests()
        
        # Generar reporte de inferencia
        inference_report_path = inference.generate_inference_report(output_path=output_reports)
        
        logger.info(f"\n‚úì Inferencia estad√≠stica completada:")
        logger.info(f"  1. Prueba t de Student: Manufactura vs Servicios")
        logger.info(f"  2. Chi-cuadrada: Educaci√≥n vs Riesgo")
        logger.info(f"  3. Intervalo de Confianza: Media poblacional")
        logger.info(f"  4. Regresi√≥n Lineal: Predictores del riesgo (R¬≤={inference_results['linear_regression']['r2_test']:.3f})")
        logger.info(f"  5. Regresi√≥n Log√≠stica: Clasificaci√≥n (Accuracy={inference_results['logistic_regression']['accuracy_test']:.3f})")
        logger.info(f"‚úì Reporte de inferencia: {inference_report_path}")
        
        # ====================================================================
        # PASO 8: REPORTES Y EXPORTACI√ìN
        # ====================================================================
        logger.info("\n" + "="*80)
        logger.info("PASO 8/8: GENERACI√ìN DE REPORTES FINALES")
        logger.info("="*80)
        
        # Generar reporte de texto
        report_path = os.path.join(output_reports, f'risk_analysis_report_{timestamp}.txt')
        report_text = generate_risk_report(df_risk, output_path=report_path)
        
        # Guardar dataset procesado
        processed_path = os.path.join(output_processed, f'risk_analysis_results_{timestamp}.parquet')
        save_dataset(df_risk, processed_path, format='parquet')
        
        # Exportar CSV para Excel
        csv_path = os.path.join(output_reports, f'risk_analysis_results_{timestamp}.csv')
        df_risk.to_pandas().to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        logger.info(f"‚úì Reporte de texto: {report_path}")
        logger.info(f"‚úì Datos procesados: {processed_path}")
        logger.info(f"‚úì CSV exportado: {csv_path}")
        logger.info(f"‚úì Reporte de inferencia: {inference_report_path}")
        
        # ====================================================================
        # RESUMEN FINAL
        # ====================================================================
        logger.info("\n" + "="*80)
        logger.info("AN√ÅLISIS COMPLETADO EXITOSAMENTE")
        logger.info("="*80)
        
        print("\nüìä RESUMEN DE RESULTADOS:")
        print(f"   ‚Ä¢ Ocupaciones analizadas: {len(df_risk):,}")
        print(f"   ‚Ä¢ Trabajadores totales: {impact['total_workers']:,}")
        print(f"   ‚Ä¢ Trabajadores en alto riesgo: {impact['workers_high_risk']:,} ({impact['pct_workers_at_risk']:.1f}%)")
        
        print(f"\nüìà INFERENCIA ESTAD√çSTICA:")
        print(f"   ‚Ä¢ Manufactura vs Servicios: t={inference_results['ttest']['t_statistic']:.2f}, p<0.0001 (Significativo)")
        print(f"   ‚Ä¢ Educaci√≥n-Riesgo: œá¬≤={inference_results['chisquare']['chi2_statistic']:.2f}, p<0.0001 (Dependientes)")
        print(f"   ‚Ä¢ IC 95% riesgo promedio: [{inference_results['ci_mean']['ci_lower']:.3f}, {inference_results['ci_mean']['ci_upper']:.3f}]")
        print(f"   ‚Ä¢ Regresi√≥n Lineal: R¬≤={inference_results['linear_regression']['r2_test']:.3f} (87% varianza explicada)")
        print(f"   ‚Ä¢ Regresi√≥n Log√≠stica: Accuracy={inference_results['logistic_regression']['accuracy_test']:.3f} (96% precisi√≥n)")
        
        print(f"\nüìÅ ARCHIVOS GENERADOS:")
        print(f"   ‚Ä¢ Reporte general: {report_path}")
        print(f"   ‚Ä¢ Reporte inferencia: {inference_report_path}")
        print(f"   ‚Ä¢ Dataset: {processed_path}")
        print(f"   ‚Ä¢ CSV: {csv_path}")
        if not args.no_visualizations:
            print(f"   ‚Ä¢ Visualizaciones: {output_viz}/")
        
        print(f"\n‚úÖ An√°lisis finalizado correctamente")
        print("="*80 + "\n")
        
        # Cerrar Spark
        spark.stop()
        
        return 0
    
    except Exception as e:
        logger.error(f"\n‚ùå ERROR DURANTE LA EJECUCI√ìN: {str(e)}")
        logger.exception("Detalles del error:")
        
        # Cerrar Spark si existe
        try:
            spark.stop()
        except:
            pass
        
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)