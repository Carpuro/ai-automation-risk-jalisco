# ğŸš€ Inicio RÃ¡pido - Quick Start

GuÃ­a de 5 minutos para empezar a usar el proyecto.

## âš¡ InstalaciÃ³n RÃ¡pida

### Paso 1: Clonar el Repositorio
```bash
git clone https://github.com/Carpuro/ai-automation-risk-jalisco.git
cd ai-automation-risk-jalisco
```

### Paso 2: Crear Entorno
```bash
# OpciÃ³n A: Con Conda (RECOMENDADO)
conda env create -f environment.yml
conda activate ai_automation_thesis

# OpciÃ³n B: Con pip
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Paso 3: Verificar InstalaciÃ³n
```bash
python verify_setup.py
```

âœ… **DeberÃ­as ver:**
```
âœ“ Python version compatible
âœ“ All dependencies installed
âœ“ Spark working correctly
âœ“ pyspark.pandas available
âœ“ Jupyter available
```

---

## ğŸ¯ 3 Formas de Usar el Proyecto

### 1ï¸âƒ£ Jupyter Notebook (RECOMENDADO para ExploraciÃ³n)

```bash
jupyter notebook
# Abrir: notebooks/automation_risk_analysis.ipynb
# Ejecutar: Kernel â†’ Restart & Run All
```

**Ventajas:**
- âœ… Interactivo
- âœ… Visualizaciones en vivo
- âœ… Documentado paso a paso
- âœ… Ideal para aprendizaje

### 2ï¸âƒ£ Script Python (Para ProducciÃ³n)

```bash
# Con datos simulados (rÃ¡pido, para pruebas)
python src/main.py --mode sample --n-occupations 200

# Con datos reales (requiere descargar O*NET + ENOE)
python src/main.py --mode real \
  --occupation-data data/raw/onet/Occupation_Data.txt \
  --employment-data data/raw/enoe_jalisco.csv
```

**Ventajas:**
- âœ… Automatizado
- âœ… Reproducible
- âœ… FÃ¡cil de programar (cron jobs)

### 3ï¸âƒ£ MÃ³dulos Individuales (Para Desarrollo)

```python
import sys
sys.path.append('src')

from data_loader import create_spark_session, load_sample_data
from automation_analyzer import AutomationRiskAnalyzer

# Crear Spark
spark = create_spark_session()

# Cargar datos
df = load_sample_data(spark, n_occupations=100)

# Analizar riesgo
analyzer = AutomationRiskAnalyzer()
df_risk = analyzer.calculate_automation_risk(df)

print(df_risk.head())
```

**Ventajas:**
- âœ… Flexible
- âœ… Ideal para experimentar
- âœ… FÃ¡cil debugging

---

## ğŸ“Š Ejemplo RÃ¡pido (5 lÃ­neas)

```python
from data_loader import create_spark_session, load_sample_data
from automation_analyzer import AutomationRiskAnalyzer

spark = create_spark_session()
df = load_sample_data(spark, n_occupations=100)
analyzer = AutomationRiskAnalyzer()
df_risk = analyzer.calculate_automation_risk(df)
print(f"Ocupaciones en alto riesgo: {(df_risk['automation_risk'] >= 0.70).sum()}")
```

---

## ğŸ¨ Generar Visualizaciones

```python
from visualizations import create_dashboard

# Genera 9 grÃ¡ficos automÃ¡ticamente
create_dashboard(df_risk, output_dir='outputs/visualizations/')
```

**Outputs:**
- `01_risk_distribution.png` - Histograma
- `02_risk_by_sector.png` - Barras por sector
- `03_salary_vs_risk.html` - Scatter interactivo
- ... (6 mÃ¡s)

---

## ğŸ“ Â¿DÃ³nde EstÃ¡n los Archivos?

```
ğŸ“¦ Proyecto
â”œâ”€â”€ ğŸ““ notebooks/automation_risk_analysis.ipynb  â† Empieza aquÃ­
â”œâ”€â”€ ğŸ src/main.py                               â† O aquÃ­ (CLI)
â”œâ”€â”€ ğŸ“Š data/sample/occupations_sample.csv        â† Datos de prueba
â””â”€â”€ ğŸ“ˆ outputs/                                  â† Resultados aquÃ­
```

---

## â“ Troubleshooting RÃ¡pido

### Error: "NumPy incompatible"
```bash
pip install "numpy<2.0" --force-reinstall
```

### Error: "PyArrow not found"
```bash
pip install pyarrow>=4.0.0
```

### Jupyter Kernel muere
```bash
# Usar Python 3.10 o 3.11 (NO 3.13)
conda create -n ai_automation_thesis python=3.10
conda activate ai_automation_thesis
pip install -r requirements.txt
```

### Visualizaciones no aparecen
```python
# En Jupyter
%matplotlib inline
import matplotlib.pyplot as plt
plt.show()
```

---

## ğŸ“š Siguientes Pasos

1. âœ… **Instalar y verificar** (arriba)
2. ğŸ““ **Ejecutar notebook** completo
3. ğŸ“Š **Ver visualizaciones** en `outputs/visualizations/`
4. ğŸ“„ **Leer documentaciÃ³n** en `docs/`
5. ğŸ”§ **Personalizar** para tus datos

---

## ğŸ“ Recursos Ãštiles

### DocumentaciÃ³n del Proyecto
- ğŸ“– [README.md](README.md) - VisiÃ³n general
- ğŸ“‹ [METHODOLOGY.md](docs/METHODOLOGY.md) - MetodologÃ­a completa
- ğŸ“Š [DATA_SOURCES.md](docs/DATA_SOURCES.md) - Fuentes de datos
- ğŸ› ï¸ [ANALYSIS_GUIDE.md](docs/ANALYSIS_GUIDE.md) - GuÃ­a paso a paso

### Tutoriales Externos
- **PySpark:** https://spark.apache.org/docs/latest/api/python/
- **O*NET:** https://www.onetcenter.org/overview.html
- **ENOE:** https://www.inegi.org.mx/programas/enoe/

---

## ğŸ’¡ Tips RÃ¡pidos

### Usar menos memoria
```python
# Reducir nÃºmero de ocupaciones
df = load_sample_data(spark, n_occupations=50)
```

### Ejecutar sin visualizaciones (mÃ¡s rÃ¡pido)
```bash
python src/main.py --mode sample --no-visualizations
```

### Ver solo top riesgos
```python
top_20 = df_risk.nlargest(20, 'automation_risk')
print(top_20[['occupation_name', 'automation_risk']])
```

---

## ğŸ¤ Â¿Necesitas Ayuda?

1. **Issues:** https://github.com/Carpuro/ai-automation-risk-jalisco/issues
2. **Email:** carlos.pulido.rosas@gmail.com
3. **DocumentaciÃ³n:** Lee `docs/ANALYSIS_GUIDE.md`

---

## âœ¨ Â¡Listo!

Ahora tienes todo para empezar. **Ejecuta el notebook** y explora:

```bash
jupyter notebook notebooks/automation_risk_analysis.ipynb
```

Â¡Buena suerte con tu anÃ¡lisis! ğŸš€

---

**Tiempo estimado:** 5-10 minutos para setup, 30-60 minutos para anÃ¡lisis completo.
